# 前置内容

## **多智能体系统基础** 

### **多智能体环境定义**  
在多智能体强化学习（MARL）中，环境根据智能体间的交互目标可分为三类：  

#### **合作型环境**  
- **定义**：智能体共享奖励函数，需**协同完成目标**（如团队足球、联合救援）  
- **关键问题**：信用分配（Credit Assignment）——如何量化单个智能体的贡献？  
- **案例**：  
  - **《星际争霸》微操战**：多个单位配合击败敌人  
  - **交通信号灯协同**：路口信号灯联合优化车流  

#### **竞争型环境**  
- **定义**：智能体目标**直接冲突**，形成零和博弈（如围棋、格斗游戏）  
- **关键问题**：纳什均衡策略——如何找到不被对手剥削的策略？  
- **案例**：  
  - **AlphaGo vs 人类**：双方策略完全对立  
  - **拍卖竞价**：投标者互相抬价  

#### **混合型环境**  
- **定义**：合作与竞争**动态共存**（如市场交易、政治谈判）  
- **关键问题**：目标对齐（Goal Alignment）——何时合作？何时竞争？  
- **案例**：  
  - **囚徒困境**：短期竞争（背叛）vs 长期合作（沉默）  
  - **《Among Us》游戏**：船员合作修飞船 vs 伪装者破坏  

---

### **经典案例分析**  

#### **囚徒困境（竞争→合作演化）**  
- **场景**：两名囚徒若互相背叛则重判，若合作则轻判  
- **MARL视角**：  
  - 单次博弈：背叛是纳什均衡（理性选择）  
  - **重复博弈**：智能体可能学会"以牙还牙"（Tit-for-Tat）策略  
  - **数学表达**：  

```python
    # 收益矩阵（Prisoner A, Prisoner B）
    payoff = {
        ('合作','合作'): (-1, -1),  # 双双轻判
        ('合作','背叛'): (-5, 0),   # A被重判，B释放
        ('背叛','合作'): (0, -5),   # A释放，B被重判
        ('背叛','背叛'): (-3, -3)   # 双双中判
    }
```

#### **足球游戏（混合型协作）**  
- **场景**：11名球员需协作进攻，但面对对手防守时转为竞争  
- **MARL挑战**：  
  - **层级策略**：底层（带球/传球）→高层（战术配合）  
  - **局部观测**：每个智能体仅看到局部视野（Partial Observability）  
  - **著名算法**：  
    - **MADDPG**：集中式训练+分布式执行  
    - **COMA**：基于反事实基线的信用分配  

---

# 标题

- multi agent 多智能体环境
- [[actor critic 行动者评价者算法]] 这是一个强化学习的框架
- cooperative competitive environments 合作竞争环境 这是多智能体在一些普适场景下会同时出现合作和竞争两种模式的场景。

# 介绍

经典的单智能体强化学习框架：
- Q-learning 基于价值学习
- policy-gradient 基于策略学习

**Q-Learning** 和 **Policy Gradient** 是强化学习（Reinforcement Learning, RL）中两种核心的算法框架，分别属于 **基于价值(Value-based)** 和 **基于策略(Policy-based)** 的方法。它们的目标都是让智能体（agent）通过与环境的交互学习到最优策略，但它们在实现方式和适用场景上有所不同。以下是对它们的详细介绍：

---

### 1. **Q-Learning**
Q-Learning 是一种**基于价值**的强化学习算法，其核心思想是通过学习**动作价值函数（Q-function）**来指导智能体的行为。

### 核心概念：
- **Q-function**：  
$Q(s, a)$表示在状态$s$下执行动作$a$后，智能体未来可能获得的累积奖励的期望值。Q-Learning 的目标是学习一个最优的 Q-function，使得智能体可以根据它选择最优动作。
- **Bellman Equation**：  
  Q-Learning 基于贝尔曼方程更新 Q-function：
$$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$
  其中：
  - $\alpha$是学习率，
  - $\gamma$是折扣因子，
  - $r$是即时奖励，
  - $s'$是下一个状态，
  - $\max_{a'} Q(s', a')$是下一个状态的最大 Q 值。

#### 特点：
- **离策略（Off-policy）**：  
  Q-Learning 在更新 Q-function 时，不依赖于智能体当前执行的策略，而是使用贪婪策略（选择最大 Q 值的动作）来更新 Q 值。
- **离散动作空间**：  
  Q-Learning 适用于离散动作空间，因为需要计算每个动作的 Q 值。
- **收敛性**：  
  在满足一定条件下（如充分探索），Q-Learning 可以收敛到最优 Q-function。

#### 优点：
- 简单直观，易于实现。
- 在离散动作空间中表现良好。

#### 缺点：
- 不适合连续动作空间。
- 对于高维状态空间，Q-function 的存储和计算成本较高（可以通过深度 Q 网络 DQN 解决）。

---

### 2. **Policy Gradient**
Policy Gradient 是一种**基于策略**的强化学习算法，其核心思想是直接优化策略$\pi(a|s)$，即智能体在状态$s$下选择动作$a$的概率分布。

#### 核心概念：
- **策略$\pi(a|s)$**：  
  策略定义了智能体在状态$s$下选择动作$a$的概率分布。Policy Gradient 的目标是通过优化策略，最大化智能体未来可能获得的累积奖励。
- **目标函数**：  
  Policy Gradient 的目标是最大化期望累积奖励：
$$
  J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T r_t \right]
$$
  其中$\tau$是轨迹（状态-动作序列），$\theta$是策略的参数。
- **梯度更新**：  
  通过计算目标函数$J(\theta)$对策略参数$\theta$的梯度，更新策略：
$$
  \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
$$
  其中$\alpha$是学习率。

#### 特点：
- **在策略（On-policy）**：  
  Policy Gradient 直接优化当前策略，因此需要根据当前策略生成的数据进行更新。
- **连续动作空间**：  
  Policy Gradient 适用于连续动作空间，因为它直接优化策略分布，而不需要为每个动作计算 Q 值。
- **随机策略**：  
  Policy Gradient 可以学习随机策略，这在某些环境中（如博弈论）可能更有利。

#### 优点：
- 适用于连续动作空间。
- 可以学习随机策略。

#### 缺点：
- 训练过程中方差较大，收敛速度较慢。
- 需要大量采样数据，效率较低（可以通过 Actor-Critic 等方法改进）。

---

由于上述内容在建模过程中都要求环境是稳定的，环境变化只因为自身的操作交互而变化。如果应用到多智能体当中，每个智能体都会进行决策，导致环境本身就是不稳定的了。这就不能够很好的利用Deep Q-Learning中的经验回放机制。同时使用Policy-Gradient又会导致方差过大，拟合度不够。

### 文章方法

- 只利用本地信息做决策。
- 不假设环境动力学的可微分模型，也不对代理之间的通信方法采取任何特定结构。
- 假设模型适用于物理行为以及通信行为。
采用中心化训练以及去中心化执行的框架训练。使用执行者-批判者模型。模型分为训练和执行两个部分。其中训练会利用到批判者的额外信息，而在执行过程中，每一个执行者只能使用到自己本地的信息。
每一名执行者在训练过程中可以利用到其他执行者的模型信息(中心化训练)，同时文章还会介绍一些方法来提高稳定性。

# 相关工作

最初的工作是使用Q-Learning的方式，只不过每一个智能体都是独立进行训练。实际上的训练效果并不好，并且经过测试，Policy-Gradient在实践中也不好。原因在于传统的Q-Learning是一个智能体对一个相同的条件进行策略的优化。但是多智能体本身在学习的过程中会改变自身的策略，对于一个独立智能体来说，相当于其他智能体在学习的同时，该智能体就要对学习了新策略的智能体进行交互，也就是在训练过程中，环境并不单一。

> Concurrently to our work, [7] proposed a similar idea of using policy gradient methods with a centralized critic, and test their approach on a StarCraft micromanagement task. Their approach differs from ours in the following ways: (1) they learn a single centralized critic for all agents, whereas we learn a centralized critic for each agent, allowing for agents with differing reward functions including competitive scenarios, (2) we consider environments with explicit communication between agents, (3) they combine recurrent policies with feed-forward critics, whereas our experiments use feed-forward policies (although our methods are applicable to recurrent policies), (4) we learn continuous policies whereas they learn discrete policies.

和当前相近的工作相比，文章提出的内容的区别在于1. 对于每一个智能体都有单独的策略奖励函数，2. 假设智能体之间的交流是在环境中显式的，说明其他智能体可以观测。3. 使用神经网络来预测动作。4. 规划空间是连续空间。

## feed-forward critics

在强化学习（Reinforcement Learning, RL）中，**feed-forward critics** 和 **feed-forward policies** 是两个不同的概念，分别对应不同的角色和功能。以下是它们的区别：

### 1. **Feed-forward Critics**
- **定义**: Feed-forward critics 是一种用于评估状态或状态-动作对的值函数（value function）或优势函数（advantage function）的神经网络。它的输入是状态（或状态-动作对），输出是对应的值（或优势）。
- **作用**: Critics 的目标是评估当前策略的表现，即估计某个状态或动作的价值。它的输出用于指导策略的更新。
- **输入**: 通常是状态$s$或状态-动作对$(s, a)$。
- **输出**: 值函数$V(s)$（状态值）或$Q(s, a)$（动作值），或者优势函数$A(s, a)$。
- **例子**: 在 Actor-Critic 框架中，Critic 就是一个 feed-forward 网络，用于估计值函数。
- **训练目标**: 最小化预测值与真实值（如 TD 目标或 Monte Carlo 回报）之间的误差。

### 2. **Feed-forward Policies**
- **定义**: Feed-forward policies 是一种用于生成动作的神经网络。它的输入是状态，输出是动作或动作的概率分布。
- **作用**: Policies 的目标是直接决定智能体在每个状态下应该采取什么动作。
- **输入**: 状态$s$。
- **输出**: 动作$a$（确定性策略）或动作的概率分布$\pi(a|s)$（随机策略）。
- **例子**: 在 Actor-Critic 框架中，Actor 就是一个 feed-forward 网络，用于生成动作。
- **训练目标**: 最大化期望回报（通过策略梯度或优化 Critic 的反馈）。

# 背景

## Markov 

### Partially Observable Markov Games

**Partially Observable Markov Games (POMGs)** 是 **Markov Games** 的扩展，用于建模多智能体系统中每个智能体只能观察到部分环境信息的情况。它是 **Partially Observable Markov Decision Processes (POMDPs)** 在多智能体环境中的推广。以下是详细解释：

---

### 1. **背景**
- **Markov Games (MGs)**: 也称为 **Stochastic Games**，是马尔可夫决策过程（MDPs）在多智能体环境中的扩展。它描述了多个智能体在同一个环境中交互，每个智能体都有自己的策略和目标。
- **Partially Observable Markov Decision Processes (POMDPs)**: 是 MDPs 的扩展，用于建模单个智能体只能观察到部分环境信息的情况。
- **Partially Observable Markov Games (POMGs)**: 结合了 MGs 和 POMDPs 的特点，用于建模多智能体系统中每个智能体只能观察到部分环境信息的情况。

---

### 2. **定义**
POMGs 可以形式化定义为以下元组：
$$
\langle N, S, \{A_i\}, \{O_i\}, P, \{R_i\}, \gamma \rangle
$$
其中：
- $N$: 智能体的集合。
- $S$: 环境状态的集合。
- $A_i$: 智能体$i$的动作空间。
- $O_i$: 智能体$i$的观察空间。
- $P$: 状态转移函数，$P(s'|s, a_1, a_2, \dots, a_N)$，表示在联合动作下环境状态从$s$转移到$s'$的概率。
- $R_i$: 智能体$i$的奖励函数，$R_i(s, a_1, a_2, \dots, a_N, s')$，表示智能体$i$在状态$s$下执行联合动作后转移到$s'$的奖励。
- $\gamma$: 折扣因子，用于计算未来奖励的现值。

---

### 3. **关键特性**
1. **部分可观测性**:
   - 每个智能体只能观察到环境的部分信息，而不是完整的状态。
   - 智能体$i$的观察$o_i$由观察函数$O_i(s)$生成。

2. **多智能体交互**:
   - 多个智能体同时采取行动，彼此的策略会相互影响。
   - 每个智能体的目标是最大化自己的累积奖励。

3. **不确定性**:
   - 由于部分可观测性，智能体无法完全确定当前状态，因此需要基于观察历史来推断环境状态。

4. **策略与信念**:
   - 智能体的策略$\pi_i$是基于其观察历史的函数。
   - 智能体可能需要维护一个信念状态（belief state），即对当前环境状态的概率分布。


## Q-Learning / Deep Q-Networks

### Q-Learning

Q-Learning 的核心思想是解决一个智能体在环境中如果有能力对一个动作的长期价值进行评估，那么它要如何学习最优的动作序列。

关键词可以总结为：

1. 离散动作空间的马尔可夫决策过程
2. Q表来实现无模型学习

#### Q表

Q 表是一个表格 (通常用矩阵表示)，用于存储每个状态-动作对的 Q 值。 Q 值表示在状态 s 下采取动作 a，并遵循最优策略所能获得的期望累积奖励。

- 行： 表示状态 s (state)
- 列： 表示动作 a (action)
- 单元格 Q(s, a)： 存储在状态 s 下执行动作 a 的预期未来奖励。

Q 表的初始值可以是任意的（通常初始化为 0 或随机值），然后通过迭代更新来逐步逼近真实的 Q 值。 更新过程基于贝尔曼方程:

$$
Q(s, a) = Q(s, a) + α * [R(s, a) + γ * max(Q(s', a')) - Q(s, a)]
$$

更新的策略会有多种，一般有：

- $\epsilon$-greedy： 随机选择动作的概率为$\epsilon$，否则选择当前最优动作。
- softmax： 基于动作概率分布的贪婪策略。
- Boltzmann 策略： 基于动作概率分布的随机策略。

在使用上就十分简单，对于当前智能体的决策，
查找给定的当前状态对应的行，选择行中最大的 Q 值对应的动作作为决策。

#### 约束

- 当状态空间增大的时候Q表是指数级膨胀
- 无法处理连续的状态或者动作
- 收敛速度依赖于更新的策略并不稳定

综上：适合网格世界、棋盘游戏等等低纬度并且是离散的问题

### Deep Q-Networks

使用深度神经网络来代表Q表，用来解决维度灾难问题。
同时引入了经验回放，打破了数据的相关性。
使用目标网络稳定训练。

核心思想是Q网络输入是观测的状态s，输出是所有动作对应的Q值。
也就是相当于Q表的一个状态到动作的映射。

#### 架构

```mermaid
graph LR
    A[状态输入] --> B[卷积神经网络]
    B --> C[全连接层]
    C --> D[各动作Q值输出]
```

#### 约束

- 过估计偏差：Q值会被高估，可以使用DDQN来解决
- 训练不稳定：调整超参数
- 依赖于离散动作：连续动作需要改进，例如NAF

## Policy Gradient (PG) Algorithms

Policy Gradient (PG) Algorithms是指策略梯度算法。
思想是直接优化策略函数，而不是间接通过价值函数。

回忆前面提到的Q-Learning。
Q-Learning是通过优化价值估计函数的准确性，
每次的策略都是选择价值最优的动作。
也就是说，实际上Q-Learning是一种基于价值来对策略进行微调的算法。

而Policy Gradient是直接优化策略函数，
直接定义一个策略函数，用来计算在某个状态s下，采取某个状态a的概率。
通过不断调整参数，来得到具体每一个动作的概率。

具体来说会包含下面一个更新策略：

$$
\nabla\theta J(\theta) = E[\nabla\theta log\pi\theta(a|s) * Q^\pi(s,a)]
$$

通俗的理解就是增加带来高回报动作的概率并且降低低回报动作的概率。

#### 优点

- 适合连续动作控制，适合机器人控制
- 可以学习随即策略，对环境要求小

#### 缺点

- 高方差：训练不稳定，使用critic作为基线函数
- 需要大量样本：可以使用经验回放和多智能体并行策略

## Deterministic Policy Gradient (DPG) Algorithms

这里的Deterministic说明了这是一个确定性的函数，
策略不再是一个概率分布。
也就是说和传统的方法不同，
这个算法学习的是一个确定性策略，而不是一个随机策略，即不会给出一个动作的概率分布。

由于使用了确定性策略，那么在概率采样的时候就可以降低方差。

# 方法

## 多智能体的Actor Critic

### 约束

- 学习策略只会使用到局部的信息，也就是每一个智能体单独的观测数据
- 不假设环境动力学的可微分模型(无法使用直接梯度下降进行参数优化)
- 不假设任何特定的交流模型(无法通过梯度下降优化通信策略)

## 推断其他智能体的策略

由于在实际环境中，很有可能并不事先了解其他智能体的策略，
那么我们就需要在每一个智能体中估计其他智能体的策略。
这里是依靠最大化似然函数做到的。

## 具有集合策略的智能体

在一般多智能体强化学习中会出现一个问题，
就是由于智能体策略的不断变化导致环境的不断变化。
同时在竞争场景下是尤为明显的，
因为智能体会过度拟合竞争对手的行为来获得强大的策略。
但是由于过拟合的问题，导致竞争对手的策略一旦改变，原来策略就失效了。
这也就是之前多智能体强化学习失败的原因。

为了减弱上述提到的过拟合问题。
文章使用了多个子策略的方法。
每一个智能体会包含有K个子策略。
第i个智能体的子策略集合表示为$\mu_{\theta_i}(k)$。
于是我们就要极大化下面的函数：

$$
J_e(\mu_i) = E_{k \sim unif(1, K), s \sim p^\mu, \alpha \sim \mu_i^{(k)}}[R_i(s,a)]
$$

其中解释一下：
- $J_e(\mu_i)$： 子策略集合$\mu_i$的期望回报
- $k \sim unif(1, K)$： 随机选择一个子策略 这里表示是一个均匀分布选择
- $s \sim p^\mu$： 状态s服从策略$\mu$下的状态分布$p^\mu$。这里的$\mu$表示所有智能体的整体策略，近似为每个智能体独立选择的策略的组合
- $a \sim \mu_i^{(k)}$： 采样动作$a$，这里使用的是子策略$\mu_i^{(k)}$。智能体i的动作a服从在给定状态s下，有k个子策略给定的动作分布
- $R_i(s,a)$： 奖励函数

由于每个不同的子策略将在不同的episode中执行，因此每一个智能体的每一个子策略都要一个经验回放缓冲区。

# 实验

## grounded communication environment

这是一种多智能体的强化学习环境。
智能体需要通过通信来完成任务，并且通信信号具有明确的物理或者环境意义。

## 具体实验环境

### Cooperative Communication

在这个环境中，智能体需要合作完成任务。
文章中是使用了speaker-listener模型。
其中listener是负责探索指定颜色的地标，根据与目标地标的距离获得奖励。
由于listener只能获取某个landmark的颜色以及相对位置，
并不确定要去到哪一个具体的landmark。
相反的，speaker就包括了对于指定的正确颜色的地标的观察，
并且会提供输出给到listener。
因此，speaker就可以从listener的操作来学习。

### Cooperative Navigation

在这个环境中，智能体是合作去到达指定的L个landmarks。
每一个智能体都可以观测到其他智能体和地标的相对位置，
并且根据其他智能体是否接近目标来进行学习。

### Keep-away

该场景由L个地标组成，包括一个目标地标，N个知道目标地标并根据其与目标的距离获得奖励的合作智能体，以及M个必须阻止合作代理到达目标的对抗智能体。
敌人通过把特工从地标上赶走，暂时占领它来实现这一点。虽然对手也会根据他们到目标地标的距离获得奖励，但他们并不知道正确的目标；这必须从智能体的运动中推断出来。

### Physical deception

在这里，N个智能体合作从总共N个地标中到达单个目标地标。
他们的奖励基于任何智能体到目标的最小距离（所以只有一个代理需要到达目标地标）。
然而，一个孤独的对手也希望到达目标地标；
陷阱是对手不知道哪一个地标是正确的。
因此，根据对手与目标的距离进行惩罚的合作智能体学习分散并覆盖所有地标以欺骗对手

### Predator-prey

在这款经典捕食者-猎物游戏的变体中，
N个速度较慢的合作智能体必须在随机生成的环境中追逐速度较快的对手，
该环境中有L个大型地标挡住前进的道路。
每次合作代理与对手发生碰撞时，
智能体都会得到奖励，而对手则会受到惩罚。
每个智能体可以观察其他智能体的相对位置和速度，以及地标的位置。

### Covert communication

这是一个对抗性通信环境，
其中说话代理（Alice）必须将消息传递给侦听代理（Bob），
后者必须在另一端重建消息。
然而，一个对抗性代理（Eve）也在观察信道，
并希望重构消息，Alice和Bob在Eve的重构基础上受到惩罚，
因此Alice必须使用一个只有Alice和Bob知道的随机生成的密钥对她的消息进行编码。

## Comparison to Decentralized Reinforcement Learning Methods
...